---
title: "Confidence Interval Convergence"
author: "Ben Buzzee"
date: "January 9, 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```


# An X% confidence interval for [a parameter] is an interval (L,U) generated by a procedure that in repeated sampling has an X% probability of containing the true value of [the parameter]. (Neyman, 1937)

In frequentist style statistics--the paradigm under which Neyman worked--probabilities are the proportion of times an outcome occurs as the number of trials tends towards infinity. So 95% confidence intervals have a 95% probability of bracketing the parameter in that as the number of trials and confidence intervals tends towards infinity, the true parameter will get bracketed by about 95% of them.

But we never create an infinite number of confidence intervals. Humans will always be working with a finite number of intervals. So does that make confidence intervals useless, or can we say more about coverage rates under the finite-interval scenario? To answer this we will conduct a simulation study.

Suppose we are a government agency tasked with making a claim about a parameter. Perhaps we work at the EPA and are interested in toxicity levels, or with Fish and Wildlife adressing the number of animals in a particular region. Each year we must estimate a parameter and draw a conclusion about that parameter. We want to be correct around 95% of the time, so we use confidence interval theory to justify our approach.


```{r}
mu <- 700
sd <- mu
num_intervals <- c(10, 25, 50, 100, 250)
contains <- data.frame()
sample_size <- 30

# for each number of intervals
for (i in 1:length(num_intervals)){
  # create intervals that many times
  for (j in 1:num_intervals[i]){
  
    # for sample sizes above 30, the distribution of the sample shouldn't matter
    samp <- rnorm(sample_size, mean = mu, sd = sd)
    est <- mean(samp)
    
    # create interval
    upper <- est + 1.96*sd(samp)/sqrt(sample_size)
    lower <- est - 1.96*sd(samp)/sqrt(sample_size)
    
    # output boolean
    contains_new <- data.frame(contains = between(mu, lower, upper), num_intervals = num_intervals[i])
    contains <- rbind(contains, contains_new)
    
  }
  
}
contains %>% group_by(num_intervals) %>% summarize(sum(contains)/n())


```


So even with a fairly high SD (mu), and if we only create 10 intervals, we almost always see at least 8 of them containing the true parameter.

